{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "771a092d",
   "metadata": {},
   "source": [
    "## Object detection, object cropping and object counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0731ac0d-4098-4a63-8126-819e59e6130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 5 persons, 1 bottle, 1 cup, 3 chairs, 2 potted plants, 1 vase, 135.0ms\n",
      "Speed: 7.1ms preprocess, 135.0ms inference, 73.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Class 'person' has 5 objects.\n",
      "Class 'vase' has 1 objects.\n",
      "Class 'potted plant' has 2 objects.\n",
      "Class 'chair' has 3 objects.\n",
      "Class 'bottle' has 1 objects.\n",
      "Class 'cup' has 1 objects.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "# Load the model\n",
    "model = YOLO(\"./models/yolov8x.pt\")\n",
    "names = model.names\n",
    "\n",
    "results = None\n",
    "\n",
    "# Load the image\n",
    "image_path = \"img2.jpg\"\n",
    "\n",
    "im0 = cv2.imread(image_path)\n",
    "# Check if the image was successfully loaded\n",
    "assert im0 is not None, \"Error reading image file\"\n",
    "\n",
    "# Create a base directory for cropped images\n",
    "crop_base_dir = \"cropped_objects\"\n",
    "if not os.path.exists(crop_base_dir):\n",
    "    os.mkdir(crop_base_dir)\n",
    "\n",
    "# Predict objects in the image\n",
    "results = model.predict(im0, show=False, device=0)\n",
    "# Extract bounding boxes and class IDs from the results\n",
    "boxes = results[0].boxes.xyxy.cpu().tolist()\n",
    "clss = results[0].boxes.cls.cpu().tolist()\n",
    "\n",
    "# Initialize a counter for cropped images\n",
    "idx = 0\n",
    "# Dictionary to keep count of objects for each class\n",
    "class_counts = {}\n",
    "\n",
    "# Check if there are any detected objects\n",
    "if boxes is not None:\n",
    "    for box, cls in zip(boxes, clss):\n",
    "        idx += 1\n",
    "\n",
    "        # Crop the detected object from the image\n",
    "        crop_obj = im0[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "\n",
    "        # Get the class name for the detected object\n",
    "        class_name = names[int(cls)]\n",
    "        # Create a directory for the class if it doesn't exist\n",
    "        class_dir = os.path.join(crop_base_dir, names[int(cls)])\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.mkdir(class_dir)\n",
    "\n",
    "        # Save the cropped object in the class directory\n",
    "        cv2.imwrite(f\"{image_path.split('.')[0]}_object_cropping_output.jpg\", im0)\n",
    "        cv2.imwrite(os.path.join(class_dir, f\"{image_path.split('.')[0]}_{idx}.png\"), crop_obj)\n",
    "\n",
    "        # Update the class count\n",
    "        if class_name in class_counts:\n",
    "            class_counts[class_name] += 1\n",
    "        else:\n",
    "            class_counts[class_name] = 1\n",
    "\n",
    "# Create an annotator for visualizing the results\n",
    "annotator = Annotator(im0, line_width=2, example=names)\n",
    "# Check if there are any detected objects\n",
    "if boxes is not None:\n",
    "    for box, cls in zip(boxes, clss):\n",
    "        # Annotate the bounding box on the image\n",
    "        annotator.box_label(box, color=colors(int(cls), True), label=names[int(cls)])\n",
    "\n",
    "# # Display the annotated image\n",
    "# cv2.imshow(\"ultralytics\", im0)\n",
    "\n",
    "# Save the annotated image\n",
    "cv2.imwrite(f\"{image_path.split('.')[0]}_object_cropping_output.jpg\", im0)\n",
    "\n",
    "# Wait for a key press and then close all OpenCV windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print the counts of objects per class\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"Class '{class_name}' has {count} objects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3378dd",
   "metadata": {},
   "source": [
    "## Face detection and facial emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43714ae9",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e4d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d5ca9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "backends = [\n",
    "  'opencv', \n",
    "  'ssd', \n",
    "  'dlib', \n",
    "  'mtcnn', \n",
    "  'fastmtcnn',\n",
    "  'retinaface', \n",
    "  'mediapipe',\n",
    "  'yolov8',\n",
    "  'yunet',\n",
    "  'centerface',\n",
    "]\n",
    "\n",
    "alignment_modes = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead02c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'emotion': {'angry': 7.590587602912535e-08,\n",
       "   'disgust': 1.5783188422663005e-15,\n",
       "   'fear': 8.595904432695534e-07,\n",
       "   'happy': 99.9997615814209,\n",
       "   'sad': 3.6539052672424077e-06,\n",
       "   'surprise': 0.00012953510122315492,\n",
       "   'neutral': 0.00010266554681948037},\n",
       "  'dominant_emotion': 'happy',\n",
       "  'region': {'x': 230,\n",
       "   'y': 43,\n",
       "   'w': 120,\n",
       "   'h': 180,\n",
       "   'left_eye': (340, 117),\n",
       "   'right_eye': (295, 117)},\n",
       "  'face_confidence': 0.79}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographies = DeepFace.analyze(\n",
    "    img_path='cropped_objects\\person\\img2_5.png',\n",
    "    detector_backend=backends[7],\n",
    "    align=alignment_modes[0],\n",
    "    actions=['emotion'],\n",
    ")\n",
    "\n",
    "demographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c1af93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common dominant emotion is 'happy'.\n"
     ]
    }
   ],
   "source": [
    "# Folder path for person class\n",
    "person_class_folder = os.path.join(crop_base_dir, 'person')\n",
    "\n",
    "# List to store dominant emotions\n",
    "dominant_emotions = []\n",
    "\n",
    "# Check if the person class folder exists\n",
    "if os.path.exists(person_class_folder):\n",
    "    # Iterate through all images in the person class folder\n",
    "    for image_name in os.listdir(person_class_folder):\n",
    "        image_path = os.path.join(person_class_folder, image_name)\n",
    "        \n",
    "        # Perform face detection and emotion recognition\n",
    "        try:\n",
    "            demographies = DeepFace.analyze(\n",
    "                img_path=image_path,\n",
    "                detector_backend=backends[7],  # RetinaFace backend\n",
    "                align=alignment_modes[0],  # Alignment enabled\n",
    "                actions=['emotion'],  # Only analyze emotions\n",
    "            )\n",
    "            # Get the dominant emotion for each image and add it to the list\n",
    "            dominant_emotion = demographies[0]['dominant_emotion']\n",
    "            dominant_emotions.append(dominant_emotion)\n",
    "            # # Print the results for each image\n",
    "            # print(f\"Results for {image_name}: {demographies}\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "# Count the occurrences of each dominant emotion\n",
    "emotion_counts = Counter(dominant_emotions)\n",
    "# Find the most common dominant emotion\n",
    "most_common_emotion = emotion_counts.most_common(1)[0][0]\n",
    "\n",
    "# Print the most common dominant emotion\n",
    "print(f\"The most common dominant emotion is '{most_common_emotion}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58156208",
   "metadata": {},
   "source": [
    "### Final code 1 (include visulization for each image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c21a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common dominant emotion is 'happy'.\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "from collections import Counter\n",
    "\n",
    "# Folder path for person class\n",
    "person_class_folder = os.path.join(crop_base_dir, 'person')\n",
    "\n",
    "# Folder path for visualization of face detection and facial emotion recognition\n",
    "emotion_visualization = \"./emotion_visualization\"\n",
    "\n",
    "# Create a directory for the class if it doesn't exist\n",
    "if not os.path.exists(emotion_visualization):\n",
    "    os.mkdir(emotion_visualization)\n",
    "\n",
    "# List to store dominant emotions\n",
    "dominant_emotions = []\n",
    "\n",
    "# Check if the person class folder exists\n",
    "if os.path.exists(person_class_folder):\n",
    "    # Iterate through all images in the person class folder\n",
    "    for image_name in os.listdir(person_class_folder):\n",
    "        image_path = os.path.join(person_class_folder, image_name)\n",
    "        \n",
    "        # Perform face detection and emotion recognition\n",
    "        try:\n",
    "            demographies = DeepFace.analyze(\n",
    "                img_path=image_path,\n",
    "                detector_backend='yolov8',  # RetinaFace backend\n",
    "                align=True,  # Alignment enabled\n",
    "                actions=['emotion'],  # Only analyze emotions\n",
    "            )\n",
    "            # Get the dominant emotion for each image and add it to the list\n",
    "            dominant_emotion = demographies[0]['dominant_emotion']\n",
    "            dominant_emotions.append(dominant_emotion)\n",
    "\n",
    "            # Visualize the face detection and emotion on the image\n",
    "            face_box = demographies[0]['region']\n",
    "            x, y, w, h = face_box['x'], face_box['y'], face_box['w'], face_box['h']\n",
    "            image = cv2.imread(image_path)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(image, dominant_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "            # Save the visualized image\n",
    "            output_image_path = os.path.join(emotion_visualization, f\"visualized_{image_name}\")\n",
    "            cv2.imwrite(output_image_path, image)\n",
    "\n",
    "            # # Print the results for each image\n",
    "            # print(f\"Results for {image_name}: {demographies}\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "# Count the occurrences of each dominant emotion\n",
    "emotion_counts = Counter(dominant_emotions)\n",
    "# Find the most common dominant emotion\n",
    "most_common_emotion = emotion_counts.most_common(1)[0][0]\n",
    "\n",
    "# Print the most common dominant emotion\n",
    "print(f\"The most common dominant emotion is '{most_common_emotion}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0cea1",
   "metadata": {},
   "source": [
    "## Final code 2 (without visualization for each image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba56d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common dominant emotion is 'happy'.\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "from collections import Counter\n",
    "\n",
    "# Folder path for person class\n",
    "person_class_folder = os.path.join(crop_base_dir, 'person')\n",
    "\n",
    "# List to store dominant emotions\n",
    "dominant_emotions = []\n",
    "\n",
    "# Check if the person class folder exists\n",
    "if os.path.exists(person_class_folder):\n",
    "    # Iterate through all images in the person class folder\n",
    "    for image_name in os.listdir(person_class_folder):\n",
    "        image_path = os.path.join(person_class_folder, image_name)\n",
    "        \n",
    "        # Perform face detection and emotion recognition\n",
    "        try:\n",
    "            demographies = DeepFace.analyze(\n",
    "                img_path=image_path,\n",
    "                detector_backend='yolov8',  # RetinaFace backend\n",
    "                align=True,  # Alignment enabled\n",
    "                actions=['emotion'],  # Only analyze emotions\n",
    "            )\n",
    "            # Get the dominant emotion for each image and add it to the list\n",
    "            dominant_emotion = demographies[0]['dominant_emotion']\n",
    "            dominant_emotions.append(dominant_emotion)\n",
    "\n",
    "            # # Print the results for each image\n",
    "            # print(f\"Results for {image_name}: {demographies}\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "# Count the occurrences of each dominant emotion\n",
    "emotion_counts = Counter(dominant_emotions)\n",
    "# Find the most common dominant emotion\n",
    "most_common_emotion = emotion_counts.most_common(1)[0][0]\n",
    "\n",
    "# Print the most common dominant emotion\n",
    "print(f\"The most common dominant emotion is '{most_common_emotion}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
